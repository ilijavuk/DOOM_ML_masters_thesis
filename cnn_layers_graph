digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140516217016880 [label="
 (64, 9216)" fillcolor=darkolivegreen1]
	140515762741840 [label=ReshapeAliasBackward0]
	140515762802448 -> 140515762741840
	140515762802448 [label=LeakyReluBackward0]
	140515762802320 -> 140515762802448
	140515762802320 [label=NativeLayerNormBackward0]
	140515762801808 -> 140515762802320
	140515762801808 [label=ConvolutionBackward0]
	140515762820624 -> 140515762801808
	140515762820624 [label=LeakyReluBackward0]
	140515762820688 -> 140515762820624
	140515762820688 [label=NativeLayerNormBackward0]
	140515762819152 -> 140515762820688
	140515762819152 [label=ConvolutionBackward0]
	140515762821136 -> 140515762819152
	140515762821136 [label=LeakyReluBackward0]
	140515762821200 -> 140515762821136
	140515762821200 [label=NativeLayerNormBackward0]
	140515762821456 -> 140515762821200
	140515762821456 [label=ConvolutionBackward0]
	140515762821712 -> 140515762821456
	140515762821712 [label=NativeLayerNormBackward0]
	140515762821776 -> 140515762821712
	140515762729904 [label="0.weight
 (3, 100, 156)" fillcolor=lightblue]
	140515762729904 -> 140515762821776
	140515762821776 [label=AccumulateGrad]
	140515762822096 -> 140515762821712
	140515762729808 [label="0.bias
 (3, 100, 156)" fillcolor=lightblue]
	140515762729808 -> 140515762822096
	140515762822096 [label=AccumulateGrad]
	140515762821904 -> 140515762821456
	140516217075664 [label="1.weight
 (32, 3, 8, 8)" fillcolor=lightblue]
	140516217075664 -> 140515762821904
	140515762821904 [label=AccumulateGrad]
	140515762821648 -> 140515762821200
	140516217075568 [label="2.weight
 (32, 24, 38)" fillcolor=lightblue]
	140516217075568 -> 140515762821648
	140515762821648 [label=AccumulateGrad]
	140515762821584 -> 140515762821200
	140516217076336 [label="2.bias
 (32, 24, 38)" fillcolor=lightblue]
	140516217076336 -> 140515762821584
	140515762821584 [label=AccumulateGrad]
	140515762821328 -> 140515762819152
	140516217075760 [label="4.weight
 (64, 32, 4, 4)" fillcolor=lightblue]
	140516217075760 -> 140515762821328
	140515762821328 [label=AccumulateGrad]
	140515762821072 -> 140515762820688
	140516217075376 [label="5.weight
 (64, 11, 18)" fillcolor=lightblue]
	140516217075376 -> 140515762821072
	140515762821072 [label=AccumulateGrad]
	140515762821008 -> 140515762820688
	140516217075856 [label="5.bias
 (64, 11, 18)" fillcolor=lightblue]
	140516217075856 -> 140515762821008
	140515762821008 [label=AccumulateGrad]
	140515762820816 -> 140515762801808
	140516217077008 [label="7.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140516217077008 -> 140515762820816
	140515762820816 [label=AccumulateGrad]
	140515762802384 -> 140515762802320
	140516217076912 [label="8.weight
 (64, 9, 16)" fillcolor=lightblue]
	140516217076912 -> 140515762802384
	140515762802384 [label=AccumulateGrad]
	140515762820560 -> 140515762802320
	140516217077104 [label="8.bias
 (64, 9, 16)" fillcolor=lightblue]
	140516217077104 -> 140515762820560
	140515762820560 [label=AccumulateGrad]
	140515762741840 -> 140516217016880
	140516217016784 [label="
 (64, 64, 9, 16)" fillcolor=darkolivegreen3]
	140515762802448 -> 140516217016784
	140516217016784 -> 140516217016880 [style=dotted]
}
