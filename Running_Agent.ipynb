{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import DoomGame  \n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##   CONSTANTS   ##\n",
    "###################\n",
    "ACTION_NUM = 3\n",
    "EPISODES_NUM = 10\n",
    "AGENT_MODEL_PATH_PREFIX = './agents/agent_for_'\n",
    "TENSORBOARD_LOG_PATH_PREFIX = './logs/logs_for_'\n",
    "CURRENT_CONFIGURATION_INDEX = 2\n",
    "\n",
    "configurations = [{\n",
    "                    'name': 'basic',\n",
    "                    'scenarioConfigFilePath': 'VizDoom/scenarios/basic.cfg',\n",
    "                    'actionNumber': 3,\n",
    "                  }, {\n",
    "                    'name': 'defend_the_center',\n",
    "                    'scenarioConfigFilePath': 'VizDoom/scenarios/defend_the_center.cfg',\n",
    "                    'actionNumber': 3,\n",
    "                  }, {\n",
    "                    'name': 'deadly_corridor',\n",
    "                    'scenarioConfigFilePath': 'VizDoom/scenarios/deadly_corridor_label_buffer.cfg',\n",
    "                    'actionNumber': 7,\n",
    "                  }, {\n",
    "                    'name': 'deathmatch',\n",
    "                    'scenarioConfigFilePath': 'VizDoom/scenarios/deathmatch.cfg',\n",
    "                    'actionNumber': 3,\n",
    "                  }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "  def __init__(self, envConfig, render=False):\n",
    "    super().__init__()\n",
    "    self.game = DoomGame()\n",
    "    self.game.load_config(envConfig[\"scenarioConfigFilePath\"])\n",
    "    self.game.set_window_visible(render)\n",
    "    self.game.init()\n",
    "\n",
    "    self.action_number = envConfig[\"actionNumber\"]\n",
    "    self.action_space = Discrete(self.action_number)\n",
    "    self.observation_space = Box(0, 255, [100, 160, 1], np.uint8)\n",
    "\n",
    "  def close(self):\n",
    "    self.game.close()\n",
    "  \n",
    "  def step(self, action):\n",
    "    actions = np.identity(self.action_number, dtype=np.uint8)\n",
    "    actionReward = self.game.make_action(actions[action], 5)\n",
    "\n",
    "    done = self.game.is_episode_finished()\n",
    "    state = self.game.get_state()\n",
    "  \n",
    "    if not state:\n",
    "      return np.zeros(self.observation_space.shape), actionReward, done, {\"damage_taken\": 0, \"hitcount\": 0, \"ammo\": 0}\n",
    "    \n",
    "    health, damage_taken, hitcount, ammo = state.game_variables\n",
    "    \n",
    "    deltasObject = {\n",
    "      'damage_taken': -damage_taken + self.rewardsObject[\"damage_taken\"],\n",
    "      'hitcount': hitcount - self.rewardsObject[\"hitcount\"],\n",
    "      'ammo': ammo - self.rewardsObject[\"ammo\"]\n",
    "    }\n",
    "    \n",
    "    self.rewardsObject[\"damage_taken\"] = damage_taken\n",
    "    self.rewardsObject[\"hitcount\"] = hitcount\n",
    "    self.rewardsObject[\"ammo\"] = ammo\n",
    "\n",
    "    reward = actionReward + deltasObject[\"damage_taken\"]*10 + deltasObject[\"hitcount\"]*200 + deltasObject[\"ammo\"]*5 \n",
    "\n",
    "    \n",
    "    \n",
    "    img = self.grayscale(state.screen_buffer)\n",
    "    # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    return img, reward, done, deltasObject\n",
    "  \n",
    "  def reset(self):\n",
    "    self.game.new_episode()\n",
    "    state = self.game.get_state()\n",
    "    self.rewardsObject = {\n",
    "      'damage_taken': 0,\n",
    "      'hitcount': 0,\n",
    "      'ammo': 52\n",
    "    }\n",
    "    return self.grayscale(state.screen_buffer)\n",
    "    \n",
    "  \n",
    "  def render():\n",
    "    pass\n",
    "  \n",
    "  def grayscale(self, observation):\n",
    "    grayscaled = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(grayscaled, (160, 100), cv2.INTER_CUBIC)\n",
    "    return np.reshape(resized, (100, 160, 1))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(configurations[CURRENT_CONFIGURATION_INDEX], render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "model = PPO.load(f\"{AGENT_MODEL_PATH_PREFIX}{configurations[CURRENT_CONFIGURATION_INDEX]['name']}/model_1000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17852\\2767318790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# clear_output()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # clear_output()\n",
    "        # plt.imshow(obs)\n",
    "        # plt.show()\n",
    "        time.sleep(0.05555)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "model = PPO.load(f\"{AGENT_MODEL_PATH_PREFIX}defend_the_center/model_best\")\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        clear_output()\n",
    "        plt.imshow(obs)\n",
    "        plt.show()\n",
    "        time.sleep(0.05)\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cba2736cce089a2bdaa6e7154cabef02622ca008eb343f0722e476d9fdc3920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
