{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EnvironmentConfigurations as EnvConfig\n",
    "\n",
    "env_params = {\n",
    "    \"env_config\": EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX],\n",
    "    \"is_reward_shaping_on\": False,\n",
    "    \"is_game_window_visible\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./agents/agent_for_deathmatch\\model_15000000 copy.zip\n"
     ]
    }
   ],
   "source": [
    "from EnvironmentHelpers import create_vectorised_environment\n",
    "from stable_baselines3 import PPO\n",
    "import glob, os\n",
    "\n",
    "model_save_path = f\"{EnvConfig.AGENT_MODEL_PATH_PREFIX}{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX]['name']}\"\n",
    "all_model_files = glob.glob(f\"{model_save_path}/*\")\n",
    "latest_model_path = max(all_model_files, key=os.path.getctime)\n",
    "\n",
    "print(f\"Loading model {latest_model_path}\")\n",
    "\n",
    "env = create_vectorised_environment(**env_params, n_envs=1)\n",
    "model = PPO.load(f\"{latest_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward:  [26.]\n",
      "Total reward:  [68.]\n",
      "Total reward:  [18.]\n",
      "Total reward:  [33.]\n",
      "Total reward:  [41.]\n",
      "Total reward:  [3.]\n",
      "Total reward:  [29.]\n",
      "Total reward:  [33.]\n",
      "Total reward:  [104.]\n",
      "Total reward:  [20.]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for episode in range(EnvConfig.EPISODES_NUM):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, second = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.05555)\n",
    "        total_reward += reward\n",
    "    print(\"Total reward: \", total_reward)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cba2736cce089a2bdaa6e7154cabef02622ca008eb343f0722e476d9fdc3920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
