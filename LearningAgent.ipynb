{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cnn import CustomCNN\n",
    "from utils.LayerActivationMonitoring import LayerActivationMonitoring, register_hooks\n",
    "from utils.LayerActivationMonitoring import plot_activations\n",
    "import EnvironmentConfigurations as EnvConfig\n",
    "\n",
    "model_save_path = f\"{EnvConfig.AGENT_MODEL_PATH_PREFIX}{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX]['name']}\"\n",
    "tensorboard_log_path = f\"{EnvConfig.TENSORBOARD_LOG_PATH_PREFIX}{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX]['name']}\"\n",
    "\n",
    "env_params = {\n",
    "    \"env_config\": EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX],\n",
    "    \"is_reward_shaping_on\": False,\n",
    "    \"is_game_window_visible\": False\n",
    "}\n",
    "\n",
    "evaluation_env_params = {\n",
    "    \"env_config\": EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX],\n",
    "    \"is_reward_shaping_on\": False,\n",
    "    \"is_game_window_visible\": False\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    \"tensorboard_log\": tensorboard_log_path,\n",
    "    \"verbose\": 1,\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"n_steps\": 8192,\n",
    "    \"clip_range\": .1,\n",
    "    \"gamma\": .95,\n",
    "    \"gae_lambda\": .9,\n",
    "    # \"n_epochs\": 3,\n",
    "    # \"n_steps\": 4096,\n",
    "    # \"learning_rate\": 1e-4,\n",
    "    # \"batch_size\": 64,\n",
    "    # \"seed\": 0,\n",
    "    # 'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The number of environments to be set is different from the number of environments in the model: (1 != 2), whereas `set_env` requires them to be the same. To load a model with a different number of environments, you must use `PPO.load(path, env)` instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16892\\3788459445.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{model_save_path}/model_480000\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0magent_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mautomatic_model_saving_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36mset_env\u001b[1;34m(self, env, force_reset)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         assert env.num_envs == self.n_envs, (\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[1;34m\"The number of environments to be set is different from the number of environments in the model: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[1;34mf\"({env.num_envs} != {self.n_envs}), whereas `set_env` requires them to be the same. To load a model with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;34mf\"a different number of environments, you must use `{self.__class__.__name__}.load(path, env)` instead\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The number of environments to be set is different from the number of environments in the model: (1 != 2), whereas `set_env` requires them to be the same. To load a model with a different number of environments, you must use `PPO.load(path, env)` instead"
     ]
    }
   ],
   "source": [
    "# RUN THE ENVIRONMENT IN PARALLEL MODE WITH 2 ENVS\n",
    "from EnvironmentHelpers import create_vectorised_environment\n",
    "from VizdoomGymWrapper import VizDoomGym\n",
    "from utils.AutomaticModelSavingCallback import AutomaticModelSavingCallback\n",
    "from utils.Initialisation import initialise_network_weights\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import policies\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# env = create_vectorised_environment(**env_params, n_envs=2)\n",
    "# evaluation_env = create_vectorised_environment(**evaluation_env_params, n_envs=1)\n",
    "env = VizDoomGym(**env_params)\n",
    "automatic_model_saving_callback = AutomaticModelSavingCallback(\n",
    "            check_freq=EnvConfig.MODEL_SAVING_FREQUENCY,\n",
    "            save_path=model_save_path)\n",
    "\n",
    "# evaluation_callback = EvalCallback(\n",
    "#             evaluation_env,\n",
    "#             n_eval_episodes=10,\n",
    "#             eval_freq=EnvConfig.EVALUATION_FREQUENCY,\n",
    "#             log_path=tensorboard_log_path,\n",
    "#             best_model_save_path=f'models/{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX][\"name\"]}')\n",
    "\n",
    "\n",
    "model = PPO(policies.ActorCriticCnnPolicy, env, device=\"cuda\", **agent_params)\n",
    "initialise_network_weights(model.policy)\n",
    "model.learn(total_timesteps=30000000, callback=[automatic_model_saving_callback])\n",
    "\n",
    "# model = PPO.load(f\"{model_save_path}/model_480000\", **agent_params)\n",
    "# model.set_env(env)\n",
    "# model.learn(total_timesteps=30000000, callback=[automatic_model_saving_callback, evaluation_callback], reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "evaluation_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ACTIVATIONS ON NEW ENVIRONMENT\n",
    "from utils.Initialisation import initialise_network_weights\n",
    "\n",
    "env = create_vectorised_environment(**env_params, n_envs=2)\n",
    "# evaluation_env = create_vectorised_environment(**env_params, n_envs=1)\n",
    "\n",
    "model = PPO(policies.ActorCriticCnnPolicy, env, **agent_params)\n",
    "\n",
    "register_hooks(model)\n",
    "initialise_network_weights(model.policy)\n",
    "\n",
    "model.learn(total_timesteps=1024, callback=[LayerActivationMonitoring()])\n",
    "\n",
    "plot_activations(model.policy.features_extractor.hooks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cba2736cce089a2bdaa6e7154cabef02622ca008eb343f0722e476d9fdc3920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
