{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cnn import CustomCNN\n",
    "from utils.LayerActivationMonitoring import LayerActivationMonitoring, register_hooks\n",
    "from utils.LayerActivationMonitoring import plot_activations\n",
    "import EnvironmentConfigurations as EnvConfig\n",
    "\n",
    "model_save_path = f\"{EnvConfig.AGENT_MODEL_PATH_PREFIX}{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX]['name']}\"\n",
    "tensorboard_log_path = f\"{EnvConfig.TENSORBOARD_LOG_PATH_PREFIX}{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX]['name']}\"\n",
    "\n",
    "env_params = {\n",
    "    \"env_config\": EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX],\n",
    "    \"is_reward_shaping_on\": False,\n",
    "    \"is_game_window_visible\": False\n",
    "}\n",
    "\n",
    "evaluation_env_params = {\n",
    "    \"env_config\": EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX],\n",
    "    \"is_reward_shaping_on\": False,\n",
    "    \"is_game_window_visible\": False\n",
    "}\n",
    "\n",
    "agent_params = {\n",
    "    \"tensorboard_log\": tensorboard_log_path,\n",
    "    \"verbose\": 1,\n",
    "    \"n_epochs\": 3,\n",
    "    \"n_steps\": 4096,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 64,\n",
    "    \"seed\": 0,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./logs/logs_for_deadly_corridor\\PPO_13\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 27   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 303  |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 664        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04307846 |\n",
      "|    clip_fraction        | 0.447      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.000124   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.52e+03   |\n",
      "|    n_updates            | 3          |\n",
      "|    policy_gradient_loss | 0.0313     |\n",
      "|    value_loss           | 3.14e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-115.99 +/- 0.01\n",
      "Episode length: 17.80 +/- 1.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -116        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010010917 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 6.56e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.23e+03    |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    value_loss           | 3.07e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 23    |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 1024  |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1383        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011579549 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.73e+03    |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    value_loss           | 2.79e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-24.63 +/- 24.32\n",
      "Episode length: 16.90 +/- 3.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.9       |\n",
      "|    mean_reward          | -24.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11587952 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1e+03      |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.0247     |\n",
      "|    value_loss           | 2.71e+03   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 23    |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 1740  |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 2087       |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09958987 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.0152     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.39e+03   |\n",
      "|    n_updates            | 15         |\n",
      "|    policy_gradient_loss | 0.0682     |\n",
      "|    value_loss           | 2.92e+03   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 2450       |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09980783 |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.43e+03   |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | 0.0669     |\n",
      "|    value_loss           | 2.74e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=16.92 +/- 26.11\n",
      "Episode length: 16.00 +/- 1.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 16.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054139882 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.59e+03    |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | 0.0413      |\n",
      "|    value_loss           | 3.1e+03     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 23    |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 2824  |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 3167        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059745196 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.55e+03    |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | 0.0399      |\n",
      "|    value_loss           | 3.6e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=14.76 +/- 59.45\n",
      "Episode length: 15.90 +/- 3.56\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.9       |\n",
      "|    mean_reward          | 14.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04788139 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.72e+03   |\n",
      "|    n_updates            | 27         |\n",
      "|    policy_gradient_loss | 0.0343     |\n",
      "|    value_loss           | 3.42e+03   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 23    |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 3528  |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 3881        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049987808 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.72e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.035       |\n",
      "|    value_loss           | 3e+03       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 4229       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04033356 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.21e+03   |\n",
      "|    n_updates            | 33         |\n",
      "|    policy_gradient_loss | 0.0215     |\n",
      "|    value_loss           | 2.91e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=70.29 +/- 48.68\n",
      "Episode length: 16.50 +/- 2.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.5        |\n",
      "|    mean_reward          | 70.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033674236 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.61e+03    |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0326      |\n",
      "|    value_loss           | 3.04e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 4591   |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 4938       |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03666982 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.14e+03   |\n",
      "|    n_updates            | 39         |\n",
      "|    policy_gradient_loss | 0.0286     |\n",
      "|    value_loss           | 3.95e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=87.97 +/- 51.41\n",
      "Episode length: 17.40 +/- 1.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.4        |\n",
      "|    mean_reward          | 88          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048145663 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.78e+03    |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.0244      |\n",
      "|    value_loss           | 3.45e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 5291   |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 5632        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048252314 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.45e+03    |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.0245      |\n",
      "|    value_loss           | 3.78e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 5967        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034980185 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.99e+03    |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.0196      |\n",
      "|    value_loss           | 4.1e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=131.52 +/- 120.36\n",
      "Episode length: 20.50 +/- 7.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | 132         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043109998 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.86e+03    |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.0294      |\n",
      "|    value_loss           | 4.03e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 6322   |\n",
      "|    total_timesteps | 147456 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 6684       |\n",
      "|    total_timesteps      | 155648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03789043 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.54e+03   |\n",
      "|    n_updates            | 54         |\n",
      "|    policy_gradient_loss | 0.0273     |\n",
      "|    value_loss           | 4.85e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=219.57 +/- 122.00\n",
      "Episode length: 28.80 +/- 8.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 28.8       |\n",
      "|    mean_reward          | 220        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04191193 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.73e+03   |\n",
      "|    n_updates            | 57         |\n",
      "|    policy_gradient_loss | 0.0222     |\n",
      "|    value_loss           | 4.2e+03    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 7052   |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 7408       |\n",
      "|    total_timesteps      | 172032     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03306745 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | 0.33       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.34e+03   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0277     |\n",
      "|    value_loss           | 4.29e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=129.83 +/- 187.49\n",
      "Episode length: 26.90 +/- 9.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.9        |\n",
      "|    mean_reward          | 130         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034740422 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.77e+03    |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.0299      |\n",
      "|    value_loss           | 4.46e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 7791   |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 8150        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049292058 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.07e+03    |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | 0.0283      |\n",
      "|    value_loss           | 4.67e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 8509        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032194696 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.52e+03    |\n",
      "|    n_updates            | 69          |\n",
      "|    policy_gradient_loss | 0.0186      |\n",
      "|    value_loss           | 4.04e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=197.27 +/- 92.47\n",
      "Episode length: 27.70 +/- 7.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.7        |\n",
      "|    mean_reward          | 197         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030208938 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.9e+03     |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    value_loss           | 4.21e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 8871   |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 9223        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026650988 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.57e+03    |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.0201      |\n",
      "|    value_loss           | 5.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=243.35 +/- 78.59\n",
      "Episode length: 32.90 +/- 6.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.9        |\n",
      "|    mean_reward          | 243         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029068064 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.67e+03    |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.0213      |\n",
      "|    value_loss           | 5.17e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 9595   |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 9948        |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030964725 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.39e+03    |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | 0.0226      |\n",
      "|    value_loss           | 4.95e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 10310       |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027606811 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.28e+03    |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | 0.0178      |\n",
      "|    value_loss           | 5.31e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=163.87 +/- 124.30\n",
      "Episode length: 27.50 +/- 9.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.5        |\n",
      "|    mean_reward          | 164         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026565857 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.62e+03    |\n",
      "|    n_updates            | 87          |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    value_loss           | 5.01e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 23     |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 10683  |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 11030       |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021119665 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.77e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0125      |\n",
      "|    value_loss           | 4.57e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=174.17 +/- 68.30\n",
      "Episode length: 29.20 +/- 4.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.2        |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022080068 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.82e+03    |\n",
      "|    n_updates            | 93          |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 4.84e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 32     |\n",
      "|    time_elapsed    | 11403  |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 11753       |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025908941 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.62e+03    |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.0216      |\n",
      "|    value_loss           | 4.74e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 12104       |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022871993 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.36e+03    |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    value_loss           | 4.68e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=217.41 +/- 96.25\n",
      "Episode length: 30.20 +/- 5.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 217         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027583562 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.99e+03    |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | 0.0077      |\n",
      "|    value_loss           | 4.96e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 35     |\n",
      "|    time_elapsed    | 12473  |\n",
      "|    total_timesteps | 286720 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 12829       |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029141296 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.41e+03    |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    value_loss           | 5.23e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=243.57 +/- 105.72\n",
      "Episode length: 31.40 +/- 6.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 31.4        |\n",
      "|    mean_reward          | 244         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022449598 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.39e+03    |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    value_loss           | 5.7e+03     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 37     |\n",
      "|    time_elapsed    | 13185  |\n",
      "|    total_timesteps | 303104 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 13530      |\n",
      "|    total_timesteps      | 311296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02074802 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.527      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.44e+03   |\n",
      "|    n_updates            | 111        |\n",
      "|    policy_gradient_loss | 0.0089     |\n",
      "|    value_loss           | 5.98e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 13884       |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015888939 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.5e+03     |\n",
      "|    n_updates            | 114         |\n",
      "|    policy_gradient_loss | 0.0182      |\n",
      "|    value_loss           | 6.5e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=223.36 +/- 122.87\n",
      "Episode length: 25.40 +/- 8.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25.4       |\n",
      "|    mean_reward          | 223        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01810906 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.507      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.12e+03   |\n",
      "|    n_updates            | 117        |\n",
      "|    policy_gradient_loss | 0.0146     |\n",
      "|    value_loss           | 6.65e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 40     |\n",
      "|    time_elapsed    | 14260  |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 14618       |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027330138 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.42e+03    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.0161      |\n",
      "|    value_loss           | 6.53e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=298.43 +/- 128.45\n",
      "Episode length: 30.40 +/- 8.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.4        |\n",
      "|    mean_reward          | 298         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024346951 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.24e+03    |\n",
      "|    n_updates            | 123         |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    value_loss           | 6.79e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 42     |\n",
      "|    time_elapsed    | 14979  |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 15329       |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026530314 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.66e+03    |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.0185      |\n",
      "|    value_loss           | 6.92e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=240.19 +/- 145.60\n",
      "Episode length: 23.70 +/- 8.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.7        |\n",
      "|    mean_reward          | 240         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021684024 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.57e+03    |\n",
      "|    n_updates            | 129         |\n",
      "|    policy_gradient_loss | 0.00946     |\n",
      "|    value_loss           | 7.32e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 44     |\n",
      "|    time_elapsed    | 15694  |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 16041       |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023004629 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.63e+03    |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    value_loss           | 7.53e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 16404       |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017482538 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.91e+03    |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | 0.0141      |\n",
      "|    value_loss           | 7.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=250.06 +/- 66.23\n",
      "Episode length: 25.10 +/- 2.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.1        |\n",
      "|    mean_reward          | 250         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017085347 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.91e+03    |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    value_loss           | 7.11e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 47     |\n",
      "|    time_elapsed    | 16772  |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 17138      |\n",
      "|    total_timesteps      | 393216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01157655 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.52e+03   |\n",
      "|    n_updates            | 141        |\n",
      "|    policy_gradient_loss | 0.0116     |\n",
      "|    value_loss           | 7.22e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=273.45 +/- 97.64\n",
      "Episode length: 26.70 +/- 6.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.7        |\n",
      "|    mean_reward          | 273         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017048124 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.61e+03    |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.00695     |\n",
      "|    value_loss           | 7.42e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 17516  |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 17860       |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015875172 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.46e+03    |\n",
      "|    n_updates            | 147         |\n",
      "|    policy_gradient_loss | 0.00879     |\n",
      "|    value_loss           | 7.26e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 18215      |\n",
      "|    total_timesteps      | 417792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01876419 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.884     |\n",
      "|    explained_variance   | 0.579      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.07e+03   |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.0132     |\n",
      "|    value_loss           | 7.12e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=283.84 +/- 139.73\n",
      "Episode length: 24.50 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.5        |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020577632 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.898      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.29e+03    |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    value_loss           | 6.98e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 18594  |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 18968       |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019743409 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.25e+03    |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | 0.00906     |\n",
      "|    value_loss           | 6.97e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=342.30 +/- 30.18\n",
      "Episode length: 23.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.6        |\n",
      "|    mean_reward          | 342         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013785407 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.7e+03     |\n",
      "|    n_updates            | 159         |\n",
      "|    policy_gradient_loss | 0.00924     |\n",
      "|    value_loss           | 7.31e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 19346  |\n",
      "|    total_timesteps | 442368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 19698       |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016685842 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.17e+03    |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | 0.00814     |\n",
      "|    value_loss           | 7.11e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 20042       |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014413953 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.52e+03    |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    value_loss           | 7.34e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=286.55 +/- 119.92\n",
      "Episode length: 22.70 +/- 5.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | 287         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 460000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015369752 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.72e+03    |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    value_loss           | 7.23e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 20404  |\n",
      "|    total_timesteps | 466944 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 20775       |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015529305 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.23e+03    |\n",
      "|    n_updates            | 171         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    value_loss           | 7e+03       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=330.55 +/- 26.44\n",
      "Episode length: 26.30 +/- 3.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.3        |\n",
      "|    mean_reward          | 331         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019095507 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.03e+03    |\n",
      "|    n_updates            | 174         |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    value_loss           | 7.11e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 21150  |\n",
      "|    total_timesteps | 483328 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 21506       |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016415007 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.726      |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.06e+03    |\n",
      "|    n_updates            | 177         |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    value_loss           | 6.8e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 21869       |\n",
      "|    total_timesteps      | 499712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012973515 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.743      |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.12e+03    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    value_loss           | 6.62e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=304.18 +/- 116.03\n",
      "Episode length: 23.00 +/- 5.16\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 23         |\n",
      "|    mean_reward          | 304        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 500000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01596252 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.751     |\n",
      "|    explained_variance   | 0.661      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.66e+03   |\n",
      "|    n_updates            | 183        |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    value_loss           | 6.69e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 22229  |\n",
      "|    total_timesteps | 507904 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 22593       |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016917376 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.92e+03    |\n",
      "|    n_updates            | 186         |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    value_loss           | 6.38e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=342.09 +/- 29.67\n",
      "Episode length: 26.90 +/- 2.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.9        |\n",
      "|    mean_reward          | 342         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019650493 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.732      |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.06e+03    |\n",
      "|    n_updates            | 189         |\n",
      "|    policy_gradient_loss | 0.00608     |\n",
      "|    value_loss           | 6.03e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 22982  |\n",
      "|    total_timesteps | 524288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 23344       |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017321438 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.58e+03    |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    value_loss           | 6.04e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=315.90 +/- 60.83\n",
      "Episode length: 26.50 +/- 4.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26.5       |\n",
      "|    mean_reward          | 316        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 540000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02440247 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.715     |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.6e+03    |\n",
      "|    n_updates            | 195        |\n",
      "|    policy_gradient_loss | 0.0134     |\n",
      "|    value_loss           | 5.98e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 23726  |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 24077       |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013311516 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.03e+03    |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | 0.00383     |\n",
      "|    value_loss           | 6.21e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 24431       |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010914678 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.06e+03    |\n",
      "|    n_updates            | 201         |\n",
      "|    policy_gradient_loss | 0.00702     |\n",
      "|    value_loss           | 6.13e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=267.84 +/- 107.98\n",
      "Episode length: 22.50 +/- 5.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.5        |\n",
      "|    mean_reward          | 268         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013832486 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.92e+03    |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    value_loss           | 6.12e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 24806  |\n",
      "|    total_timesteps | 565248 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 25160       |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017770983 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.49e+03    |\n",
      "|    n_updates            | 207         |\n",
      "|    policy_gradient_loss | 0.00599     |\n",
      "|    value_loss           | 5.93e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=345.78 +/- 17.49\n",
      "Episode length: 27.40 +/- 1.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 27.4       |\n",
      "|    mean_reward          | 346        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 580000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02761128 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.582     |\n",
      "|    explained_variance   | 0.716      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.22e+03   |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0082     |\n",
      "|    value_loss           | 5.55e+03   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 25529  |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 25892       |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011920264 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.72e+03    |\n",
      "|    n_updates            | 213         |\n",
      "|    policy_gradient_loss | 0.00522     |\n",
      "|    value_loss           | 5.5e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 26261       |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009938518 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.12e+03    |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | 0.00513     |\n",
      "|    value_loss           | 5.57e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=296.72 +/- 137.00\n",
      "Episode length: 24.30 +/- 6.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.3        |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014354812 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.03e+03    |\n",
      "|    n_updates            | 219         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    value_loss           | 5.42e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 26641  |\n",
      "|    total_timesteps | 606208 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 27003        |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142852245 |\n",
      "|    clip_fraction        | 0.0881       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.58e+03     |\n",
      "|    n_updates            | 222          |\n",
      "|    policy_gradient_loss | 0.0102       |\n",
      "|    value_loss           | 5.15e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=327.71 +/- 102.89\n",
      "Episode length: 26.90 +/- 5.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26.9         |\n",
      "|    mean_reward          | 328          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097283125 |\n",
      "|    clip_fraction        | 0.0876       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.73e+03     |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | 0.00629      |\n",
      "|    value_loss           | 5.21e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 27378  |\n",
      "|    total_timesteps | 622592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 27742       |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012446102 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.56e+03    |\n",
      "|    n_updates            | 228         |\n",
      "|    policy_gradient_loss | 0.00287     |\n",
      "|    value_loss           | 5.42e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 28100       |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008718919 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.42e+03    |\n",
      "|    n_updates            | 231         |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    value_loss           | 5.19e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=324.08 +/- 100.20\n",
      "Episode length: 26.00 +/- 3.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26         |\n",
      "|    mean_reward          | 324        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 640000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00952553 |\n",
      "|    clip_fraction        | 0.0802     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.542     |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.44e+03   |\n",
      "|    n_updates            | 234        |\n",
      "|    policy_gradient_loss | 0.00605    |\n",
      "|    value_loss           | 4.94e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 28468  |\n",
      "|    total_timesteps | 647168 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 28831       |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007926537 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.3e+03     |\n",
      "|    n_updates            | 237         |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    value_loss           | 4.99e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=299.75 +/- 126.82\n",
      "Episode length: 24.00 +/- 4.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24          |\n",
      "|    mean_reward          | 300         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013252507 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.25e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00524     |\n",
      "|    value_loss           | 4.96e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 29197  |\n",
      "|    total_timesteps | 663552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 29560       |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011445571 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.11e+03    |\n",
      "|    n_updates            | 243         |\n",
      "|    policy_gradient_loss | 0.00493     |\n",
      "|    value_loss           | 4.87e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 29909       |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010436187 |\n",
      "|    clip_fraction        | 0.0764      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.32e+03    |\n",
      "|    n_updates            | 246         |\n",
      "|    policy_gradient_loss | 0.00477     |\n",
      "|    value_loss           | 4.73e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=315.04 +/- 109.84\n",
      "Episode length: 25.70 +/- 5.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.7        |\n",
      "|    mean_reward          | 315         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017179526 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.93e+03    |\n",
      "|    n_updates            | 249         |\n",
      "|    policy_gradient_loss | 0.00339     |\n",
      "|    value_loss           | 4.43e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 30293  |\n",
      "|    total_timesteps | 688128 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 30655       |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010504238 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.87e+03    |\n",
      "|    n_updates            | 252         |\n",
      "|    policy_gradient_loss | 0.0069      |\n",
      "|    value_loss           | 4.5e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=316.45 +/- 117.74\n",
      "Episode length: 24.20 +/- 5.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 24.2       |\n",
      "|    mean_reward          | 316        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 700000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00825566 |\n",
      "|    clip_fraction        | 0.0731     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.493     |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.9e+03    |\n",
      "|    n_updates            | 255        |\n",
      "|    policy_gradient_loss | 0.00684    |\n",
      "|    value_loss           | 4.4e+03    |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 31028  |\n",
      "|    total_timesteps | 704512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 31387       |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008350645 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.3e+03     |\n",
      "|    n_updates            | 258         |\n",
      "|    policy_gradient_loss | 0.007       |\n",
      "|    value_loss           | 4.26e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=291.68 +/- 135.66\n",
      "Episode length: 26.10 +/- 6.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.1        |\n",
      "|    mean_reward          | 292         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006710954 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.95e+03    |\n",
      "|    n_updates            | 261         |\n",
      "|    policy_gradient_loss | 0.00518     |\n",
      "|    value_loss           | 4.13e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 31760  |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 32124      |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00758063 |\n",
      "|    clip_fraction        | 0.0622     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.519     |\n",
      "|    explained_variance   | 0.791      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.53e+03   |\n",
      "|    n_updates            | 264        |\n",
      "|    policy_gradient_loss | 0.00253    |\n",
      "|    value_loss           | 4.14e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 32480       |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008486178 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.2e+03     |\n",
      "|    n_updates            | 267         |\n",
      "|    policy_gradient_loss | 0.00368     |\n",
      "|    value_loss           | 3.97e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=352.80 +/- 29.72\n",
      "Episode length: 28.10 +/- 1.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 28.1         |\n",
      "|    mean_reward          | 353          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 740000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135798715 |\n",
      "|    clip_fraction        | 0.0866       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.03e+03     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.00693      |\n",
      "|    value_loss           | 4.27e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 32854  |\n",
      "|    total_timesteps | 745472 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 33218       |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014268376 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.89e+03    |\n",
      "|    n_updates            | 273         |\n",
      "|    policy_gradient_loss | 0.00567     |\n",
      "|    value_loss           | 4.05e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=270.27 +/- 131.46\n",
      "Episode length: 24.20 +/- 7.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | 270         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026068 |\n",
      "|    clip_fraction        | 0.0765      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.63e+03    |\n",
      "|    n_updates            | 276         |\n",
      "|    policy_gradient_loss | 0.00785     |\n",
      "|    value_loss           | 3.95e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 33595  |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 33952       |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010117218 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.52e+03    |\n",
      "|    n_updates            | 279         |\n",
      "|    policy_gradient_loss | 0.00454     |\n",
      "|    value_loss           | 3.92e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 34299      |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01188058 |\n",
      "|    clip_fraction        | 0.0728     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.514     |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.57e+03   |\n",
      "|    n_updates            | 282        |\n",
      "|    policy_gradient_loss | 0.00483    |\n",
      "|    value_loss           | 3.67e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=337.80 +/- 20.93\n",
      "Episode length: 26.80 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | 338         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191161 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.58e+03    |\n",
      "|    n_updates            | 285         |\n",
      "|    policy_gradient_loss | 0.00402     |\n",
      "|    value_loss           | 3.43e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 34676  |\n",
      "|    total_timesteps | 786432 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 35029       |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009870784 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.26e+03    |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | 0.00351     |\n",
      "|    value_loss           | 3.27e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=329.45 +/- 77.63\n",
      "Episode length: 27.40 +/- 3.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 27.4       |\n",
      "|    mean_reward          | 329        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 800000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01209946 |\n",
      "|    clip_fraction        | 0.0958     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.451     |\n",
      "|    explained_variance   | 0.814      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.65e+03   |\n",
      "|    n_updates            | 291        |\n",
      "|    policy_gradient_loss | 0.0094     |\n",
      "|    value_loss           | 3.46e+03   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 35391  |\n",
      "|    total_timesteps | 802816 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 35748       |\n",
      "|    total_timesteps      | 811008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009713689 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.65e+03    |\n",
      "|    n_updates            | 294         |\n",
      "|    policy_gradient_loss | 0.0026      |\n",
      "|    value_loss           | 3.28e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 36111       |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009709617 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.62e+03    |\n",
      "|    n_updates            | 297         |\n",
      "|    policy_gradient_loss | 0.00439     |\n",
      "|    value_loss           | 3.39e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=329.27 +/- 72.06\n",
      "Episode length: 27.70 +/- 3.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27.7         |\n",
      "|    mean_reward          | 329          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 820000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071030473 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.5e+03      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.00178      |\n",
      "|    value_loss           | 3.17e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 101    |\n",
      "|    time_elapsed    | 36510  |\n",
      "|    total_timesteps | 827392 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 36860       |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010903364 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.81e+03    |\n",
      "|    n_updates            | 303         |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    value_loss           | 2.99e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=327.45 +/- 115.64\n",
      "Episode length: 26.00 +/- 5.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26           |\n",
      "|    mean_reward          | 327          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052472604 |\n",
      "|    clip_fraction        | 0.052        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.72e+03     |\n",
      "|    n_updates            | 306          |\n",
      "|    policy_gradient_loss | 0.0043       |\n",
      "|    value_loss           | 3.08e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 103    |\n",
      "|    time_elapsed    | 37226  |\n",
      "|    total_timesteps | 843776 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 37578       |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011056316 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.88e+03    |\n",
      "|    n_updates            | 309         |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 2.88e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=363.03 +/- 17.80\n",
      "Episode length: 26.90 +/- 2.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.9        |\n",
      "|    mean_reward          | 363         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007308122 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.85e+03    |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | 0.00242     |\n",
      "|    value_loss           | 2.77e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 105    |\n",
      "|    time_elapsed    | 37952  |\n",
      "|    total_timesteps | 860160 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 38311       |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006917394 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 998         |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | 0.00568     |\n",
      "|    value_loss           | 2.69e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 38666        |\n",
      "|    total_timesteps      | 876544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067278286 |\n",
      "|    clip_fraction        | 0.0463       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.434       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.05e+03     |\n",
      "|    n_updates            | 318          |\n",
      "|    policy_gradient_loss | 0.00535      |\n",
      "|    value_loss           | 2.82e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=300.42 +/- 138.14\n",
      "Episode length: 26.00 +/- 6.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26           |\n",
      "|    mean_reward          | 300          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 880000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077539403 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 969          |\n",
      "|    n_updates            | 321          |\n",
      "|    policy_gradient_loss | 0.0041       |\n",
      "|    value_loss           | 3.06e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 39043  |\n",
      "|    total_timesteps | 884736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 39405       |\n",
      "|    total_timesteps      | 892928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010886561 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.03e+03    |\n",
      "|    n_updates            | 324         |\n",
      "|    policy_gradient_loss | 0.00931     |\n",
      "|    value_loss           | 2.54e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=368.84 +/- 39.26\n",
      "Episode length: 25.90 +/- 1.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.9        |\n",
      "|    mean_reward          | 369         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009164405 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.41e+03    |\n",
      "|    n_updates            | 327         |\n",
      "|    policy_gradient_loss | 0.0014      |\n",
      "|    value_loss           | 2.32e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 39770  |\n",
      "|    total_timesteps | 901120 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 40137        |\n",
      "|    total_timesteps      | 909312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054294546 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 629          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.00395      |\n",
      "|    value_loss           | 2.33e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 40482       |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006124747 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 626         |\n",
      "|    n_updates            | 333         |\n",
      "|    policy_gradient_loss | 0.00517     |\n",
      "|    value_loss           | 2.5e+03     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=335.82 +/- 41.09\n",
      "Episode length: 25.90 +/- 2.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.9        |\n",
      "|    mean_reward          | 336         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005164573 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.04e+03    |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | 0.0035      |\n",
      "|    value_loss           | 2.54e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 113    |\n",
      "|    time_elapsed    | 40851  |\n",
      "|    total_timesteps | 925696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 41209        |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053411713 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 776          |\n",
      "|    n_updates            | 339          |\n",
      "|    policy_gradient_loss | 0.0028       |\n",
      "|    value_loss           | 2.33e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=251.66 +/- 175.11\n",
      "Episode length: 21.80 +/- 8.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.8        |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005228348 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.31e+03    |\n",
      "|    n_updates            | 342         |\n",
      "|    policy_gradient_loss | 0.00502     |\n",
      "|    value_loss           | 2.21e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 115    |\n",
      "|    time_elapsed    | 41589  |\n",
      "|    total_timesteps | 942080 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 41941        |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052434593 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.435       |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.23e+03     |\n",
      "|    n_updates            | 345          |\n",
      "|    policy_gradient_loss | 0.00146      |\n",
      "|    value_loss           | 2.08e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 42307        |\n",
      "|    total_timesteps      | 958464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077404096 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.63e+03     |\n",
      "|    n_updates            | 348          |\n",
      "|    policy_gradient_loss | 0.00212      |\n",
      "|    value_loss           | 2.38e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=346.74 +/- 121.91\n",
      "Episode length: 25.40 +/- 5.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25.4         |\n",
      "|    mean_reward          | 347          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 960000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113600865 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 773          |\n",
      "|    n_updates            | 351          |\n",
      "|    policy_gradient_loss | 0.00795      |\n",
      "|    value_loss           | 1.84e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 42679  |\n",
      "|    total_timesteps | 966656 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 43041       |\n",
      "|    total_timesteps      | 974848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013830677 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 710         |\n",
      "|    n_updates            | 354         |\n",
      "|    policy_gradient_loss | 0.00339     |\n",
      "|    value_loss           | 2.22e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=368.07 +/- 15.09\n",
      "Episode length: 29.00 +/- 2.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29          |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005535594 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.07e+03    |\n",
      "|    n_updates            | 357         |\n",
      "|    policy_gradient_loss | 0.00195     |\n",
      "|    value_loss           | 1.9e+03     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 22     |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 43428  |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 43791       |\n",
      "|    total_timesteps      | 991232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008751254 |\n",
      "|    clip_fraction        | 0.0681      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.07e+03    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00691     |\n",
      "|    value_loss           | 2.02e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 44148       |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005855416 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.422      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 652         |\n",
      "|    n_updates            | 363         |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=338.51 +/- 32.19\n",
      "Episode length: 27.70 +/- 2.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.7        |\n",
      "|    mean_reward          | 339         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005569913 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 961         |\n",
      "|    n_updates            | 366         |\n",
      "|    policy_gradient_loss | 0.00266     |\n",
      "|    value_loss           | 1.81e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 123     |\n",
      "|    time_elapsed    | 44521   |\n",
      "|    total_timesteps | 1007616 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 44871       |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006640731 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.397      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 660         |\n",
      "|    n_updates            | 369         |\n",
      "|    policy_gradient_loss | 0.00288     |\n",
      "|    value_loss           | 1.73e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=257.05 +/- 146.14\n",
      "Episode length: 23.50 +/- 7.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 23.5       |\n",
      "|    mean_reward          | 257        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1020000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00637135 |\n",
      "|    clip_fraction        | 0.0589     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.411     |\n",
      "|    explained_variance   | 0.875      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 993        |\n",
      "|    n_updates            | 372        |\n",
      "|    policy_gradient_loss | 0.00734    |\n",
      "|    value_loss           | 1.98e+03   |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 125     |\n",
      "|    time_elapsed    | 45262   |\n",
      "|    total_timesteps | 1024000 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 45617       |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007649813 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 673         |\n",
      "|    n_updates            | 375         |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    value_loss           | 1.84e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=289.88 +/- 129.22\n",
      "Episode length: 23.90 +/- 5.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 23.9         |\n",
      "|    mean_reward          | 290          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1040000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074094115 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 774          |\n",
      "|    n_updates            | 378          |\n",
      "|    policy_gradient_loss | 0.00554      |\n",
      "|    value_loss           | 1.7e+03      |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 127     |\n",
      "|    time_elapsed    | 45986   |\n",
      "|    total_timesteps | 1040384 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 128        |\n",
      "|    time_elapsed         | 46339      |\n",
      "|    total_timesteps      | 1048576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01011094 |\n",
      "|    clip_fraction        | 0.0446     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.412     |\n",
      "|    explained_variance   | 0.896      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 890        |\n",
      "|    n_updates            | 381        |\n",
      "|    policy_gradient_loss | 0.00323    |\n",
      "|    value_loss           | 1.76e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 46718       |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006995432 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 571         |\n",
      "|    n_updates            | 384         |\n",
      "|    policy_gradient_loss | 0.00742     |\n",
      "|    value_loss           | 1.91e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=318.26 +/- 113.98\n",
      "Episode length: 25.40 +/- 5.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25.4       |\n",
      "|    mean_reward          | 318        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1060000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879797 |\n",
      "|    clip_fraction        | 0.0648     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.4       |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 551        |\n",
      "|    n_updates            | 387        |\n",
      "|    policy_gradient_loss | 0.00465    |\n",
      "|    value_loss           | 1.61e+03   |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 130     |\n",
      "|    time_elapsed    | 47094   |\n",
      "|    total_timesteps | 1064960 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 47454        |\n",
      "|    total_timesteps      | 1073152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078562815 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 671          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | 0.00293      |\n",
      "|    value_loss           | 1.78e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=280.21 +/- 143.64\n",
      "Episode length: 24.20 +/- 7.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | 280         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006783263 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.12e+03    |\n",
      "|    n_updates            | 393         |\n",
      "|    policy_gradient_loss | 0.00418     |\n",
      "|    value_loss           | 1.69e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 132     |\n",
      "|    time_elapsed    | 47837   |\n",
      "|    total_timesteps | 1081344 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 48199       |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008001351 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.44e+03    |\n",
      "|    n_updates            | 396         |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    value_loss           | 1.78e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 48568       |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327797 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 545         |\n",
      "|    n_updates            | 399         |\n",
      "|    policy_gradient_loss | 0.0044      |\n",
      "|    value_loss           | 1.68e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=306.33 +/- 87.71\n",
      "Episode length: 25.90 +/- 4.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25.9         |\n",
      "|    mean_reward          | 306          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1100000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076119294 |\n",
      "|    clip_fraction        | 0.0514       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 354          |\n",
      "|    n_updates            | 402          |\n",
      "|    policy_gradient_loss | 0.00385      |\n",
      "|    value_loss           | 1.78e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 135     |\n",
      "|    time_elapsed    | 48950   |\n",
      "|    total_timesteps | 1105920 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 49311       |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005758701 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 527         |\n",
      "|    n_updates            | 405         |\n",
      "|    policy_gradient_loss | 0.0022      |\n",
      "|    value_loss           | 1.58e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=248.64 +/- 170.95\n",
      "Episode length: 21.80 +/- 8.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.8        |\n",
      "|    mean_reward          | 249         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006265692 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 563         |\n",
      "|    n_updates            | 408         |\n",
      "|    policy_gradient_loss | 0.00494     |\n",
      "|    value_loss           | 1.67e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 137     |\n",
      "|    time_elapsed    | 49682   |\n",
      "|    total_timesteps | 1122304 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 138        |\n",
      "|    time_elapsed         | 50029      |\n",
      "|    total_timesteps      | 1130496    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00975107 |\n",
      "|    clip_fraction        | 0.0466     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.37      |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 834        |\n",
      "|    n_updates            | 411        |\n",
      "|    policy_gradient_loss | 0.00208    |\n",
      "|    value_loss           | 1.82e+03   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 139          |\n",
      "|    time_elapsed         | 50396        |\n",
      "|    total_timesteps      | 1138688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074757715 |\n",
      "|    clip_fraction        | 0.0657       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 425          |\n",
      "|    n_updates            | 414          |\n",
      "|    policy_gradient_loss | 0.00713      |\n",
      "|    value_loss           | 1.36e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=361.95 +/- 19.02\n",
      "Episode length: 27.20 +/- 2.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.2        |\n",
      "|    mean_reward          | 362         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006258781 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 596         |\n",
      "|    n_updates            | 417         |\n",
      "|    policy_gradient_loss | 0.00264     |\n",
      "|    value_loss           | 1.36e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 140     |\n",
      "|    time_elapsed    | 50778   |\n",
      "|    total_timesteps | 1146880 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 141          |\n",
      "|    time_elapsed         | 51140        |\n",
      "|    total_timesteps      | 1155072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058390894 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 427          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | 0.00321      |\n",
      "|    value_loss           | 1.75e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=325.56 +/- 55.05\n",
      "Episode length: 25.80 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.8        |\n",
      "|    mean_reward          | 326         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011699317 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 352         |\n",
      "|    n_updates            | 423         |\n",
      "|    policy_gradient_loss | 0.00439     |\n",
      "|    value_loss           | 1.2e+03     |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 142     |\n",
      "|    time_elapsed    | 51523   |\n",
      "|    total_timesteps | 1163264 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 51884       |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005877753 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 550         |\n",
      "|    n_updates            | 426         |\n",
      "|    policy_gradient_loss | 0.00435     |\n",
      "|    value_loss           | 1.27e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 52256       |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008242458 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 488         |\n",
      "|    n_updates            | 429         |\n",
      "|    policy_gradient_loss | 0.00385     |\n",
      "|    value_loss           | 1.31e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=333.79 +/- 81.55\n",
      "Episode length: 26.60 +/- 4.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26.6       |\n",
      "|    mean_reward          | 334        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00788546 |\n",
      "|    clip_fraction        | 0.0603     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.341     |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 387        |\n",
      "|    n_updates            | 432        |\n",
      "|    policy_gradient_loss | 0.00274    |\n",
      "|    value_loss           | 1.27e+03   |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 145     |\n",
      "|    time_elapsed    | 52652   |\n",
      "|    total_timesteps | 1187840 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 53011       |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010431534 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.327      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.04e+03    |\n",
      "|    n_updates            | 435         |\n",
      "|    policy_gradient_loss | 0.00441     |\n",
      "|    value_loss           | 1.45e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=321.31 +/- 85.65\n",
      "Episode length: 24.80 +/- 3.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 24.8         |\n",
      "|    mean_reward          | 321          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063180597 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 300          |\n",
      "|    n_updates            | 438          |\n",
      "|    policy_gradient_loss | 0.00457      |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 147     |\n",
      "|    time_elapsed    | 53394   |\n",
      "|    total_timesteps | 1204224 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 53756        |\n",
      "|    total_timesteps      | 1212416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074660243 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 278          |\n",
      "|    n_updates            | 441          |\n",
      "|    policy_gradient_loss | 0.00621      |\n",
      "|    value_loss           | 1.19e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=321.27 +/- 83.43\n",
      "Episode length: 27.20 +/- 4.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27.2         |\n",
      "|    mean_reward          | 321          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1220000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110783465 |\n",
      "|    clip_fraction        | 0.054        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 544          |\n",
      "|    n_updates            | 444          |\n",
      "|    policy_gradient_loss | 0.00331      |\n",
      "|    value_loss           | 1.67e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 149     |\n",
      "|    time_elapsed    | 54150   |\n",
      "|    total_timesteps | 1220608 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 54498       |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009678664 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 719         |\n",
      "|    n_updates            | 447         |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    value_loss           | 1.38e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 151          |\n",
      "|    time_elapsed         | 54864        |\n",
      "|    total_timesteps      | 1236992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055391253 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 511          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | 0.00483      |\n",
      "|    value_loss           | 1.62e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=285.30 +/- 130.95\n",
      "Episode length: 23.40 +/- 5.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 23.4         |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1240000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068658376 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 919          |\n",
      "|    n_updates            | 453          |\n",
      "|    policy_gradient_loss | 0.00418      |\n",
      "|    value_loss           | 1.56e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 152     |\n",
      "|    time_elapsed    | 55229   |\n",
      "|    total_timesteps | 1245184 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 55580       |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008177017 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.36e+03    |\n",
      "|    n_updates            | 456         |\n",
      "|    policy_gradient_loss | 0.00388     |\n",
      "|    value_loss           | 1.58e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=353.90 +/- 14.71\n",
      "Episode length: 27.00 +/- 1.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27           |\n",
      "|    mean_reward          | 354          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1260000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060075987 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 675          |\n",
      "|    n_updates            | 459          |\n",
      "|    policy_gradient_loss | 0.0021       |\n",
      "|    value_loss           | 1.52e+03     |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 154     |\n",
      "|    time_elapsed    | 55950   |\n",
      "|    total_timesteps | 1261568 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 56305       |\n",
      "|    total_timesteps      | 1269760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008743083 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 321         |\n",
      "|    n_updates            | 462         |\n",
      "|    policy_gradient_loss | 0.00342     |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 56674        |\n",
      "|    total_timesteps      | 1277952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053333594 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 805          |\n",
      "|    n_updates            | 465          |\n",
      "|    policy_gradient_loss | 0.00442      |\n",
      "|    value_loss           | 1.81e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=376.90 +/- 35.93\n",
      "Episode length: 28.40 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 28.4         |\n",
      "|    mean_reward          | 377          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1280000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052856538 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 406          |\n",
      "|    n_updates            | 468          |\n",
      "|    policy_gradient_loss | 0.00454      |\n",
      "|    value_loss           | 1.71e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 157     |\n",
      "|    time_elapsed    | 57048   |\n",
      "|    total_timesteps | 1286144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 57418       |\n",
      "|    total_timesteps      | 1294336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008835702 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.295      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 471         |\n",
      "|    policy_gradient_loss | 0.00226     |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=348.96 +/- 32.62\n",
      "Episode length: 25.70 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.7        |\n",
      "|    mean_reward          | 349         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005947289 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 375         |\n",
      "|    n_updates            | 474         |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    value_loss           | 1.02e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 159     |\n",
      "|    time_elapsed    | 57800   |\n",
      "|    total_timesteps | 1302528 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 58158        |\n",
      "|    total_timesteps      | 1310720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057773334 |\n",
      "|    clip_fraction        | 0.0361       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 413          |\n",
      "|    n_updates            | 477          |\n",
      "|    policy_gradient_loss | 0.00375      |\n",
      "|    value_loss           | 1.16e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 161          |\n",
      "|    time_elapsed         | 58504        |\n",
      "|    total_timesteps      | 1318912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047406508 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.289       |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 550          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | 0.00454      |\n",
      "|    value_loss           | 1.4e+03      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=366.55 +/- 20.17\n",
      "Episode length: 27.10 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.1        |\n",
      "|    mean_reward          | 367         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008878792 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 817         |\n",
      "|    n_updates            | 483         |\n",
      "|    policy_gradient_loss | 0.00457     |\n",
      "|    value_loss           | 1.52e+03    |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 162     |\n",
      "|    time_elapsed    | 58885   |\n",
      "|    total_timesteps | 1327104 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 163        |\n",
      "|    time_elapsed         | 59246      |\n",
      "|    total_timesteps      | 1335296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00902644 |\n",
      "|    clip_fraction        | 0.0558     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.304     |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 289        |\n",
      "|    n_updates            | 486        |\n",
      "|    policy_gradient_loss | 0.00301    |\n",
      "|    value_loss           | 1.37e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=356.27 +/- 24.55\n",
      "Episode length: 27.50 +/- 1.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.5        |\n",
      "|    mean_reward          | 356         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010233798 |\n",
      "|    clip_fraction        | 0.078       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 607         |\n",
      "|    n_updates            | 489         |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    value_loss           | 1.7e+03     |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 22      |\n",
      "|    iterations      | 164     |\n",
      "|    time_elapsed    | 59627   |\n",
      "|    total_timesteps | 1343488 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 59990       |\n",
      "|    total_timesteps      | 1351680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475078 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.295      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 530         |\n",
      "|    n_updates            | 492         |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    value_loss           | 1.61e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 60354        |\n",
      "|    total_timesteps      | 1359872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083818715 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 326          |\n",
      "|    n_updates            | 495          |\n",
      "|    policy_gradient_loss | 0.00765      |\n",
      "|    value_loss           | 1.48e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=328.74 +/- 59.02\n",
      "Episode length: 26.40 +/- 2.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26.4         |\n",
      "|    mean_reward          | 329          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1360000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060831197 |\n",
      "|    clip_fraction        | 0.0465       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 350          |\n",
      "|    n_updates            | 498          |\n",
      "|    policy_gradient_loss | 0.00703      |\n",
      "|    value_loss           | 1.18e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15084\\3580963821.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActorCriticCnnPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0magent_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0minitialise_network_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mautomatic_model_saving_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# model = PPO.load(f\"{model_save_path}/model_400000\", **agent_params)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         )\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Transpose the terminal observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Programs\\Anaconda\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             )\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Masters\\DOOM_ML\\VizdoomGymWrapper.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0maction_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN THE ENVIRONMENT IN PARALLEL MODE WITH 2 ENVS\n",
    "from EnvironmentHelpers import create_vectorised_environment\n",
    "from utils.AutomaticModelSavingCallback import AutomaticModelSavingCallback  \n",
    "from utils.Initialisation import initialise_network_weights\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import policies\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "env = create_vectorised_environment(**env_params, n_envs=2)\n",
    "evaluation_env = create_vectorised_environment(**evaluation_env_params, n_envs=1)\n",
    "\n",
    "automatic_model_saving_callback = AutomaticModelSavingCallback(\n",
    "            check_freq=EnvConfig.MODEL_SAVING_FREQUENCY,\n",
    "            save_path=model_save_path)\n",
    "\n",
    "evaluation_callback = EvalCallback(\n",
    "            evaluation_env, \n",
    "            n_eval_episodes=10, \n",
    "            eval_freq=EnvConfig.EVALUATION_FREQUENCY,\n",
    "            log_path=tensorboard_log_path,\n",
    "            best_model_save_path=f'models/{EnvConfig.configurations[EnvConfig.CURRENT_CONFIGURATION_INDEX][\"name\"]}')\n",
    "\n",
    "\n",
    "model = PPO(policies.ActorCriticCnnPolicy, env, device=\"cuda\", **agent_params)\n",
    "initialise_network_weights(model.policy)\n",
    "model.learn(total_timesteps=30000000, callback=[automatic_model_saving_callback, evaluation_callback])\n",
    "\n",
    "# model = PPO.load(f\"{model_save_path}/model_400000\", **agent_params)\n",
    "# model.set_env(env)\n",
    "# model.learn(total_timesteps=30000000, callback=[automatic_model_saving_callback, evaluation_callback], reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "evaluation_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ACTIVATIONS ON NEW ENVIRONMENT\n",
    "from utils.Initialisation import initialise_network_weights\n",
    "\n",
    "env = create_vectorised_environment(**env_params, n_envs=2)\n",
    "# evaluation_env = create_vectorised_environment(**env_params, n_envs=1)\n",
    "\n",
    "model = PPO(policies.ActorCriticCnnPolicy, env, **agent_params)\n",
    "\n",
    "register_hooks(model)\n",
    "initialise_network_weights(model.policy)\n",
    "\n",
    "model.learn(total_timesteps=1024, callback=[LayerActivationMonitoring()])\n",
    "\n",
    "plot_activations(model.policy.features_extractor.hooks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cba2736cce089a2bdaa6e7154cabef02622ca008eb343f0722e476d9fdc3920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
